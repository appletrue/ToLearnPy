python字符串编码



# 一、标准ASCII编码:

上世纪60年代，美国制定的一套字符编码，把英文字母，数字和常用标点符号每个字符做了统一编码，一共128个，使用至今。    
编码值小于128    
0 ~ 127     2**7 = 128
2**（3+3+1）= 2**3 .2**3.2**1=8.8.2=128
1字节表示 0 ~ 2**7 -1   (0 ~ 127)
0b0xxxxxxx （1个字节的最高位是0）

在linux下查看ASCII，使用命令：
$ man ascii<回车>
（前32个为非打印字符，用于控制）

##  常用的ASCII编码:
| 字符   | 十进制  | 十六进制 |
| ---- | ---- | ---- |
| '0'  | 48   | 0x30 |
| 'A'  | 65   | 0x41 |
| 'a'  | 97   | 0x61 |

# 二、扩展ASCII编码：

一些欧洲国家的字符带有注音符号，比如法语中的é，那么使用标准ASCII码就不能表示出来。
后来，这些国家就把标准ASCII码中最高的位置1，又扩展出来128个编码来用。

编码值大于127，小于256    
1字节表示  2**7 ~ 2**8 - 1 (128 ~ 255)    
0b1xxxxxxx （1个字节的最高位是1）

但是这样又带来新的问题，欧洲国家很多，每个国家都用到扩展ASCII码值，造成了编码冲突。比如码值130，在法语中代表é，在希伯来语中却代表了Gimel (ג)，在俄语编码中又代表另一个符号。这样，扩展ASCII还是显得不够用！

到了亚洲国家，符号更多，一个字节的ASCII编码根本不够用，那怎么办？干脆再扩展出一个字节来！比如简体中文常见编码方式GB2312，就是两个字节表示一个汉子，理论上可以表示出256x256 = 65536个符号。

# 三、Unicode编码

上述类似两个字节的编码方式虽然可以表示较多符号，但是依然存在问题：    
1，还是不够用，比如汉字就超过10万左右。这样就需要更大字节数的编码。    
2，编码方式繁多，导致一个问题：就是当对一个文件或电子邮件操作时，必须先知道其编码格式，否则打开之后就是常看到的“乱码”。

- 解决办法：
  定义一个广泛的编码集合，把世界上所有符号纳入统一编码。
  yeah！
  这就是unicode的初衷。
  现在的unicode的规模可以容纳100多万个符号。    
  具体的符号对应表，可以查询unicode.org，或者专门的汉字对应表（http://www.chi2ko.com/tool/CJK.htm）

- unicode的问题
  需要注意的是，unicode只是编码集，只是给每一个符号分配了一个唯一的id数值，并没有定义出来如何存储。

比如，汉字 好 的 Unicode 是十六进制数597d，这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。

问题1，需要的字节数不固定，比如汉字就超过10万，虽然可以采用3字节表示，但是计算机如何区别3个字节是表示的一个汉字而不是三个独立的其他编码？
比如“好”的编码值是597d，分开看59是ascii的'Y', 7d是ascii的'}'。

问题2，如果unicode做统一规定，比如每个符号固定用3或4个字节表示，那么每个英文或欧洲大多数国家的字符只用了1个字节，其他2-3个字节就是存0，这样文本大小会超出2-3倍，显然不合理.如何处理？

# 四、utf-8编码：

utf8就是在互联网上使用最广的一种Unicode的实现方式（存储方法）。其他实现方式还包括UTF16（字符用两个字节或四个字节表示）和UTF32（字符用四个字节表示），不过在互联网上基本不用。

- utf-8规则：
  1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。    
  2）对于n字节的符号（n > 1），第一个字节的前n位都设为1，第n + 1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。     
  下表总结了编码规则，字母x表示可用编码的位。
| Unicode符号范围         | UTF-8编码方式                           |
| ------------------- | ----------------------------------- |
| (十六进制)              | （二进制）                               |
| 0000 0000-0000 007F | 0xxxxxxx                            |
| 0000 0080-0000 07FF | 110xxxxx 10xxxxxx                   |
| 0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx          |
| 0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx |

# 五、gb2312编码：
- 规定: 一个小于127的字符的意义与原来相同, 但两个大于127的字符连在一起时, 就表示一个汉字, 前面的一个字节(他称之为高字节)从0xA1用到 0xF7, 后面一个字节(低字节)从0xA1到0xFE, 这样我们就可以组合出大约7000多个简体汉字了. 在这些编码里, 我们还 把数学符号,罗马希腊的 字母,日文的假名们都编进去了, 连在 ASCII 里本来就有的数字,标点,字母都统统重新编了两个字节长的编码, 这就是常说的"全角"字符, 而原来在127号以下的那些就叫"半角"字符了。中国人民看到这样很不错, 于是就把这种汉字方案叫做 "GB2312". GB2312 是对 ASCII的中文扩展.

# 六、GBK编码：
GBK即“国标”、“扩展”汉语拼音的第一个字母，英文名称：Chinese Internal Code Specification，但是中国的汉字太多了, 我们很快就就发现有许多人名和地名没有办法在这里打出来,于是我们不得不继续把GB2312有用到的码位找出来豪不客气地用上.后来还是不够用, 于是干脆不再要求低字节一定是127号之后的内码, 只要第一个字节是大于127就固定表示这是一个汉字的开始, 不管后面跟的是不是扩展字 符集里的内容. 结果扩展之后的编码方案被称为 "GBK" 标准, GBK 包括了 GB2312 的所有内容, 同时又增加了近20000个新的汉字(包括繁体字)和符号.

显然，GBK编码是可以存放足够多的汉字，但是跟utf-8编码格式却不兼容。

# python 中的uniconde，utf-8，gbk转换的处理

python　中使用Unicode作用内部编码！

Unicode 16(两字节表示法)     
   'A'  0x00A1    
Unicode 32(四字节表示法)  0 ~ 2**32-1(约40亿)    
   'A'  0x000000A1

python源码中默认存储方式是ASCII，如果定义的字符串中使用了超过127的编码，必须指定一种编码方式（对于汉字，linux下用utf-8，windows下用gbk）才能正常使用。
指定编码的方法是：
第一种，在源码文件的第一行，写如下：
```python
# -*- coding:utf-8 -*-
或
# coding=utf-8
# -*- coding:gbk -*-
或
# coding=gbk
```

第二种，写如下：

```python
import sys
sys.setdefaultencoding('utf-8')
或
sys.setdefaultencoding('gbk')
```

转换图示意如下：

```
utf-8 ==》 unicode ==》 gbk
^^                      ||
||                      ||
unicode <==============	vv
```

示例代码：

```python
string='你好'
print("string is:",type(string))
print(string)

ustring=u"你好"
print("ustring is:",type(ustring))
print(ustring)

gbkstring=ustring.encode("gbk")
print("gbkstring is:",type(gbkstring))
print(gbkstring)

anotherstring=gbkstring.decode("gbk")
print("anotherstring is:",type(anotherstring))
print(anotherstring)

#输出结果如下：
string is: <class 'str'>
你好
ustring is: <class 'str'>
你好
gbkstring is: <class 'bytes'>
b'\xc4\xe3\xba\xc3'
anotherstring is: <class 'str'>
你好
```